
# All about attentions

## Where does 'attention' comes from?

## Linear attention

## Properties of attention

### Sparsity?


# References

1. [Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention](https://proceedings.mlr.press/v119/katharopoulos20a.html?ref=mackenziemorehead.com)
2. [Efficient Attention: Attention With Linear Complexities](https://openaccess.thecvf.com/content/WACV2021/html/Shen_Efficient_Attention_Attention_With_Linear_Complexities_WACV_2021_paper.html)
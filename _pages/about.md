---
layout: about
title: about
permalink: /

profile:
  align: right
  image: qinyuan.JPG
  image_circular: false # crops the image to make it circular
  more_info: >
    <p>qwu [at] mpi-sws [dot] org</p>
    <p>Campus E1 5</p>
    <p>66125, Saarbruecken, Germany </p>

news: true # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page

#Edit `_bibliography/papers.bib` and Jekyll will render your [publications page](/al-folio/publications/) automatically.

#Link to your social media connections, too. This theme is set up to use [Font Awesome icons](https://fontawesome.com/) and [Academicons](https://jpswalsh.github.io/academicons/), like the ones below. Add your Facebook, Twitter, LinkedIn, Google Scholar, or just disable all of them.

---

I am a third-year PhD student at the <a href="https://www.cis.mpg.de/">CS@Max Planck</a> and the <a href="https://www.mpi-sws.org/">Max Planck Institute for Software Systems (MPI-SWS)</a>, advised by <a href="https://people.mpi-sws.org/~gummadi/">Krishna Gummadi</a>. I am also fortunate to closely collaborate with and receive guidance from <a href="https://cs-people.bu.edu/evimaria/">Evimaria Terzi</a> (Boston University), <a href="https://mtoneva.com/">Mariya Toneva</a> (MPI-SWS), and <a href="https://informatik.rub.de/zafar/">Muhammad Bilal Zafar</a> (Ruhr University Bochum) (Odered by last name alphabet). Before I joined MPI-SWS, I got my bachelor degree in mathematics-physics from <a href="https://en.uestc.edu.cn/"> University of Electronic Science and Technology of China (UESTC)</a>.

I investigate how large language models (LLMs) internalize, represent, and utilize knowledge—seeking to enhance their reliability, interpretability, and safety. My work centers on understanding the interplay between internal learning (from training) and external adaptation (via prompts, retrieval, or tool use).

Beyond core research, I collaborate on:

1. Privacy and security in LLMs – balancing data protection with model utility and efficiency.

2. Neuroscience-inspired modeling – linking human memory mechanisms to LLM cognition.

3. LLM systems and optimization – exploring how PEFT, quantization, and inference techniques affect learning and behavior.

Ultimately, I aim to close the loop between how LLMs learn, remember, and act—toward more trustworthy and cognitively grounded AI systems.

---

![Overview of my research](assets/img/overview_work.png){: style="display:block; margin-left:auto; margin-right:auto; width:90%; border-radius:10px;" }

<p style="text-align:center; font-style:italic;">Figure: Overview of my research focus — connecting internal and external knowledge in LLMs.</p>
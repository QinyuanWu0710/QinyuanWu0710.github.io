---
layout: about
title: about
permalink: /

profile:
  align: right
  image: qinyuan.JPG
  image_circular: false # crops the image to make it circular
  more_info: >
    <p>qwu [at] mpi-sws [dot] org</p>
    <p>Campus E1 5</p>
    <p>66125, Saarbruecken, Germany </p>

news: true # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page

#Edit `_bibliography/papers.bib` and Jekyll will render your [publications page](/al-folio/publications/) automatically.

#Link to your social media connections, too. This theme is set up to use [Font Awesome icons](https://fontawesome.com/) and [Academicons](https://jpswalsh.github.io/academicons/), like the ones below. Add your Facebook, Twitter, LinkedIn, Google Scholar, or just disable all of them.

---


Here’s a revised version of your bio:

I am a third-year PhD student at the <a href="https://www.cis.mpg.de/">CS Department at Max Planck</a> and the <a href="https://www.mpi-sws.org/">Max Planck Institute for Software Systems (MPI-SWS)</a>, advised by <a href="https://people.mpi-sws.org/~gummadi/">Krishna Gummadi</a>. I am also fortunate to closely collaborate with and receive guidance from <a href="https://cs-people.bu.edu/evimaria/">Evimaria Terzi</a>, <a href="https://mtoneva.com/">Mariya Toneva</a>, and <a href="https://informatik.rub.de/zafar/">Muhammad Bilal Zafar</a>.

My research focuses on understanding the inner workings of deep learning models to build more reliable and transparent AI systems, with an emphasis on Large Language Models (LLMs). Specifically, I investigate how these models encode, retrieve, and process information, delving into questions like memorization, latent knowledge estimation and knowledge injection of LLMs. My goal is to uncover insights that will enable the development of dependable and interpretable deep learning systems.

Beyond this, I am enthusiastic about collaborating on:

1. *Privacy and security challenges in LLMs* – exploring ways to mitigate risks while maintaining model utility.
2. *The intersection of neuroscience and language models* – investigating how insights from the human brain can inform AI research and vice versa.
3. *Systems for serving LLMs* – including Parameter-Efficient Fine-Tuning (PEFT), quantization, and inference optimization methods like KV caching. While not an expert in LLM systems, I find it fascinating to explore how these optimizations influence model behavior.

<!-- Prior to joining MPI-SWS, I earned my bachelor's degree in mathematics-physics fundamental science from the <a href="https://www.yingcai.uestc.edu.cn//">Yingcai Honors College</a> at the <a href="https://en.uestc.edu.cn/">University of Electronic Science and Technology of China (UESTC)</a>. -->
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h1 id="overview-of-how-to-build-ai-agents">Overview of How to Build AI Agents</h1> <p>This reading note is a curated summary of papers and blog posts on AI agents. Many texts are directly quoted or adapted from the original sources. The primary goal is to consolidate and summarize key resources on AI agents for easy reference and study. Updates will be added as new developments arise.</p> <p><em>Note: Touch-ups by chatGPT.</em></p> <p><em>Qinyuan Wu, last updated: 2025.01.15</em></p> <hr> <ul> <li> <a href="#ai-agents">AI Agents</a> <ul> <li><a href="#what-are-agents">What Are Agents?</a></li> <li> <a href="#improving-reasoning-and-planning-abilities">Improving Reasoning and Planning Abilities</a> <ul> <li><a href="#task-decomposition">Task Decomposition</a></li> <li><a href="#self-reflection">Self-Reflection</a></li> <li><a href="#cognitive-architecture-memory-for-reasoning-and-planning">Cognitive Architecture: Memory for Reasoning and Planning</a></li> </ul> </li> <li> <a href="#using-tools">Using Tools</a> <ul> <li><a href="#data-stores">Data Stores</a></li> <li> <a href="#external-apis">External APIs</a> <ul> <li><a href="#extensions">Extensions</a></li> <li><a href="#functions">Functions</a></li> </ul> </li> <li><a href="#workflows">Workflows</a></li> <li><a href="#model-context-protocol-mcp">Model Context Protocol (MCP)</a></li> </ul> </li> <li><a href="#environments">Environments</a></li> <li><a href="#enhancing-model-performance-with-targeted-learning">Enhancing Model Performance with Targeted Learning</a></li> <li><a href="#case-studies">Case Studies</a></li> <li><a href="#resources">Resources</a></li> </ul> </li> </ul> <hr> <h2 id="what-are-agents">What Are Agents?</h2> <p>What’s the difference between AI agents and AI models?</p> <ul> <li> <strong>Models</strong>: Trained systems that do not alter their parameters in the environment, such as pre-trained or fine-tuned LLMs. They serve as centralized decision-makers for agent processes.</li> <li> <strong>Workflows</strong>: Systems where LLMs and tools are orchestrated through predefined code paths. Tools bridge the gap between foundational models and external data/services.</li> <li> <strong>Agents</strong>: Systems where LLMs dynamically manage their processes and tool usage, maintaining control over how they accomplish tasks.</li> </ul> <div style="text-align: center;"> <img src="/assets/images/image-6.png" alt="alt text" style="width: 90%;"> <p><em>Overview of an AI agent, figure from <a href="https://www.anthropic.com/research/building-effective-agents" rel="external nofollow noopener" target="_blank">Building Effective Agents</a></em></p> </div> <p>Agents achieve their goals using cognitive architectures that process information iteratively, make informed decisions, and refine actions based on previous outputs.</p> <h3 id="key-points">Key Points:</h3> <ol> <li>Enabling the LLM’s <strong>reasoning capabilities</strong> to make good decisions: <ul> <li> <em>Task decomposition</em>: Frameworks like Chain-of-Thought (CoT) and Tree-of-Thought (ToT) are helpful.</li> <li> <em>Self-reflection</em>: Allow agents to iteratively refine decisions and correct mistakes.</li> </ul> </li> <li>Ensuring the decision-making LLM <strong>uses the right tools</strong>.</li> <li>Providing <strong>feedback</strong> from the environment and determining when to stop iterating.</li> </ol> <h2 id="improving-reasoning-and-planning-abilities">Improving Reasoning and Planning Abilities</h2> <h3 id="task-decomposition">Task Decomposition</h3> <ol> <li> <p><strong><a href="https://arxiv.org/abs/2201.11903" rel="external nofollow noopener" target="_blank">Chain of Thought (CoT)</a></strong>: A standard prompting technique instructing models to “think step by step,” breaking complex tasks into simpler steps. CoT enhances performance by utilizing more test-time computation and making the model’s reasoning process interpretable.</p> </li> <li> <p><strong><a href="https://arxiv.org/abs/2305.10601" rel="external nofollow noopener" target="_blank">Tree of Thoughts (ToT)</a></strong>: Extends CoT by exploring multiple reasoning possibilities at each step. Problems are decomposed into thought steps, generating multiple thoughts per step, forming a tree structure. Searches can use BFS (breadth-first search) or DFS (depth-first search), with states evaluated by classifiers or majority vote.</p> </li> </ol> <h3 id="self-reflection">Self-Reflection</h3> <ol> <li> <strong><a href="https://arxiv.org/pdf/2210.03629" rel="external nofollow noopener" target="_blank">ReAct</a></strong>: Combines reasoning and acting by expanding the action space to include task-specific actions and natural language reasoning traces. This enables interaction with the environment (e.g., using APIs) while documenting the reasoning process.</li> </ol> <div style="text-align: center;"> <img src="/assets/images/ReAct.png" alt="alt text" style="width: 80%;"> <p><em>Figure from <a href="https://arxiv.org/pdf/2210.03629" rel="external nofollow noopener" target="_blank">ReAct: Synergizing Reasoning and Acting in Language Models</a></em></p> </div> <ol> <li> <strong><a href="https://arxiv.org/abs/2303.11366" rel="external nofollow noopener" target="_blank">Reflexion</a></strong>: A framework equipping agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion uses a reinforcement learning setup where the reward model provides binary rewards, and actions follow the ReAct structure, incorporating task-specific actions and language-based reasoning.</li> </ol> <div style="text-align: center;"> <img src="/assets/images/image-8.png" alt="alt text" style="width: 80%;"> <p><em>Figure from <a href="https://arxiv.org/abs/2303.11366" rel="external nofollow noopener" target="_blank">Reflexion: Language Agents with Verbal Reinforcement Learning</a></em></p> </div> <ol> <li><strong><a href="https://arxiv.org/abs/2302.02676" rel="external nofollow noopener" target="_blank">Chain of Hindsight</a></strong></li> <li><strong><a href="https://arxiv.org/abs/2210.14215" rel="external nofollow noopener" target="_blank">Algorithm Distillation</a></strong></li> </ol> <h3 id="cognitive-architecture-memory-for-reasoning-and-planning">Cognitive Architecture: Memory for Reasoning and Planning</h3> <p>Memory plays a crucial role in an agent’s reasoning process:</p> <div style="text-align: center;"> <img src="/assets/images/cognitive_arch_agents.png" alt="alt text" style="width: 40%;"> <p><em>Figure from <a href="https://arxiv.org/abs/2309.02427" rel="external nofollow noopener" target="_blank">Cognitive Architectures for Language Agents</a></em></p> </div> <p><strong>Human Brain vs. Agent Memory</strong></p> <p>Human cognitive architecture broadly includes:</p> <div style="text-align: center;"> <img src="/assets/images/image-9.png" alt="alt text" style="width: 80%;"> <p><em>Figure from <a href="https://lilianweng.github.io/posts/2023-06-23-agent/" rel="external nofollow noopener" target="_blank">Lil’s log: LLM Powered Autonomous Agents</a></em></p> </div> <ol> <li> <strong>Sensory Memory</strong>: Learning embedding representations for raw inputs (e.g., text, images).</li> <li> <strong>Short-Term Memory</strong>: In-context learning, limited by the finite context window of transformers.</li> <li> <strong>Long-Term Memory</strong>: External vector stores for fast retrieval during query time.</li> </ol> <p>Some researchers suggest aligning an agent’s cognitive architecture with the human brain’s.</p> <div style="text-align: center;"> <img src="/assets/images/CoALA.png" alt="alt text" style="width: 80%;"> <p><em>Figure from <a href="https://arxiv.org/abs/2309.02427" rel="external nofollow noopener" target="_blank">Cognitive Architectures for Language Agents</a></em></p> </div> <h2 id="using-tools">Using Tools</h2> <h3 id="data-stores">Data Stores</h3> <p>Data stores convert incoming documents into vector database embeddings, allowing agents to extract necessary information. For example, Retrieval-Augmented Generation (RAG) uses vector embeddings to retrieve contextually relevant information.</p> <div style="text-align: center;"> <img src="/assets/images/data_stores.png" alt="alt text" style="width: 80%;"> <p><em>Figure from <a href="https://www.kaggle.com/whitepaper-agents" rel="external nofollow noopener" target="_blank">Google White Book: Agents</a></em></p> </div> <h3 id="external-apis">External APIs</h3> <ol> <li> <strong>Tool Design</strong>: Tools should be clearly defined and well-documented, with prompt engineering as detailed as the overall model prompts. Recommendations from <a href="https://www.anthropic.com/research/building-effective-agents" rel="external nofollow noopener" target="_blank">Building Effective Agents</a>: <ul> <li>Give the model enough tokens to “think” before committing to decisions.</li> <li>Use natural formats familiar to the model.</li> <li>Avoid excessive formatting overhead.</li> </ul> </li> <li> <strong>Enhancing Model Performance</strong>: Discussed further in the next section.</li> </ol> <h4 id="extensions">Extensions</h4> <p>Extensions bridge the gap between agents and APIs by teaching the agent:</p> <ol> <li>How to use API endpoints with examples.</li> <li>What arguments or parameters are required for successful API calls.</li> </ol> <div style="text-align: center;"> <img src="/assets/images/extensions.png" alt="alt text" style="width: 90%;"> <p><em>Figure from <a href="https://www.kaggle.com/whitepaper-agents" rel="external nofollow noopener" target="_blank">Google White Book: Agents</a></em></p> </div> <h4 id="functions">Functions</h4> <p>Functions provide developers more control over API execution and data flow. For instance, a user requesting a ski trip suggestion might involve:</p> <div style="text-align: center;"> <img src="/assets/images/ski_model.png" alt="Ski Model" style="width: 80%;"> <p><em>Initial output from Ski Model</em></p> </div> <p>The model’s output can be structured in JSON for easier parsing:</p> <div style="text-align: center;"> <img src="/assets/images/ski_json.png" alt="Ski JSON" style="width: 80%;"> <p><em>Ski Model Output in JSON Format</em></p> </div> <p>This allows for better API usage:</p> <div style="text-align: center;"> <img src="/assets/images/ski_agent.png" alt="Ski Agent" style="width: 80%;"> <p><em>Ski Agent - External API Integration</em></p> </div> <h3 id="workflows">Workflows</h3> <p>To integrate data stores and APIs in an agent system, developers build workflows using LLMs and tools. Examples include:</p> <div style="text-align: center;"> <img src="/assets/images/image-1.png" alt="Prompt chaining" style="width: 80%;"> <p><em>Prompt chaining with programmatic checks</em></p> </div> <div style="text-align: center;"> <img src="/assets/images/image-2.png" alt="Routing" style="width: 80%;"> <p><em>Routing</em></p> </div> <div style="text-align: center;"> <img src="/assets/images/image-3.png" alt="Parallelization" style="width: 80%;"> <p><em>Parallelization</em></p> </div> <div style="text-align: center;"> <img src="/assets/images/image-4.png" alt="Orchestrator-workers" style="width: 80%;"> <p><em>Orchestrator-workers</em></p> </div> <div style="text-align: center;"> <img src="/assets/images/image-5.png" alt="Evaluator-optimizer" style="width: 80%;"> <p><em>Evaluator-optimizer</em></p> </div> <h3 id="model-context-protocol-mcp">Model Context Protocol (MCP)</h3> <p>The <a href="https://www.anthropic.com/news/model-context-protocol" rel="external nofollow noopener" target="_blank">Model Context Protocol (MCP)</a> establishes secure connections to external systems like content repositories and business tools, ensuring models produce relevant, safe responses.</p> <h2 id="environments">Environments</h2> <p>Agents interact with their environments as “text games,” receiving textual observations and producing textual actions.</p> <ol> <li> <strong>Physical Environments</strong>: AI interacts with the physical world via perceptual inputs (e.g., vision, audio) converted into text and robotic planners executing commands.</li> <li> <strong>Dialogue Environments</strong>: Agents engage in linguistic interactions, assisting with tasks or collaborating with other agents in simulations, debates, or problem-solving.</li> <li> <strong>Digital Environments</strong>: AI operates in virtual platforms like APIs or websites, augmenting knowledge and computation in cost-effective, testable settings.</li> </ol> <h2 id="enhancing-model-performance-with-targeted-learning">Enhancing Model Performance with Targeted Learning</h2> <p>Strategies to improve model tool selection:</p> <ol> <li> <strong>In-Context Learning</strong>: Example-based inference (e.g., ReAct framework).</li> <li> <strong>Retrieval-Based In-Context Learning</strong>: Dynamically retrieves relevant information or tools from external memory.</li> <li> <strong>Fine-Tuning</strong>: Pre-training on larger, specific datasets for enhanced performance.</li> </ol> <h2 id="case-studies">Case Studies</h2> <ul> <li><strong><a href="https://agentlaboratory.github.io/" rel="external nofollow noopener" target="_blank">Agent Laboratory: Using LLM Agents as Research Assistants</a></strong></li> <li><strong><a href="https://arxiv.org/abs/2412.17767" rel="external nofollow noopener" target="_blank">ResearchTown: Simulator of Human Research Community</a></strong></li> </ul> <hr> <h2 id="resources">Resources</h2> <h3 id="research-papers">Research Papers</h3> <ul> <li><a href="https://arxiv.org/abs/2201.11903" rel="external nofollow noopener" target="_blank">Chain of Thought Prompting</a></li> <li><a href="https://arxiv.org/abs/2305.10601" rel="external nofollow noopener" target="_blank">Tree of Thoughts</a></li> <li><a href="https://arxiv.org/pdf/2210.03629" rel="external nofollow noopener" target="_blank">ReAct</a></li> <li><a href="https://arxiv.org/abs/2303.11366" rel="external nofollow noopener" target="_blank">Reflexion</a></li> <li><a href="https://arxiv.org/abs/2302.02676" rel="external nofollow noopener" target="_blank">Chain of Hindsight</a></li> <li><a href="https://arxiv.org/abs/2210.14215" rel="external nofollow noopener" target="_blank">Algorithm Distillation</a></li> <li><a href="https://arxiv.org/abs/2309.02427" rel="external nofollow noopener" target="_blank">Cognitive Architectures for Language Agents</a></li> </ul> <h3 id="blog-posts-and-reports">Blog Posts and Reports</h3> <ul> <li><a href="https://lilianweng.github.io/posts/2023-06-23-agent/" rel="external nofollow noopener" target="_blank">Lil’s Log: LLM Powered Autonomous Agents</a></li> <li><a href="https://www.anthropic.com/research/building-effective-agents" rel="external nofollow noopener" target="_blank">Building Effective Agents</a></li> <li><a href="https://www.kaggle.com/whitepaper-agents" rel="external nofollow noopener" target="_blank">Agents</a></li> <li><a href="https://www.promptingguide.ai/research/llm-agents" rel="external nofollow noopener" target="_blank">Prompt Engineering Guide</a></li> </ul> </body></html>